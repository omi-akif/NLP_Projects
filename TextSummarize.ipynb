{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed UAT for Chatbot Localisation project settings and created tickets accordingly\n",
      "Updated KTs and Card for BNY-Eid Campaign and introduced special button for campaign offers and mystery box\n",
      "Updated KTs and Card for BNY-Eid Campaign\n",
      "Updated Native Knowledge Base for Local chatbot platform\n",
      "Performed UATs for Chatbot localisation project and raised ticket accordingly.Updated KTs and Card for Eid Shopping Fest Campaign, and raised ticket for failed cases for Chatbot localisation project\n",
      "Local Chatbot launched\n",
      "Configured Briefing List and Shortcut buttons on Local chatbot platform Configured Greetings and Talk to Agent Card for Local chatbot platform\n",
      "Performed UAT for new entry point from Local DD entry point on 28th April - this is the first time a chatbot has entered into contact with an agent since launch of Local DD App.\n",
      "Indexes of top ranked_sentence order are  [(0.1665970201019602, ['Updated', 'Native', 'Knowledge', 'Base', 'for', 'Local', 'chatbot', 'platform']), (0.16291993983934508, ['Configured', 'Briefing', 'List', 'and', 'Shortcut', 'buttons', 'on', 'Local', 'chatbot', 'platform', 'Configured', 'Greetings', 'and', 'Talk', 'to', 'Agent', 'Card', 'for', 'Local', 'chatbot', 'platform']), (0.1531093631136772, ['Local', 'Chatbot', 'launched']), (0.14544993379564614, ['Performed', 'UATs', 'for', 'Chatbot', 'localisation', 'project', 'and', 'raised', 'ticket', 'accordingly.Updated', 'KTs', 'and', 'Card', 'for', 'Eid', 'Shopping', 'Fest', 'Campaign,', 'and', 'raised', 'ticket', 'for', 'failed', 'cases', 'for', 'Chatbot', 'localisation', 'project']), (0.14041396609047105, ['Updated', 'KTs', 'and', 'Card', 'for', 'BNY-Eid', 'Campaign']), (0.12189928476720326, ['Updated', 'KTs', 'and', 'Card', 'for', 'BNY-Eid', 'Campaign', 'and', 'introduced', 'special', 'button', 'for', 'campaign', 'offers', 'and', 'mystery', 'box']), (0.10961049229169714, ['Performed', 'UAT', 'for', 'Chatbot', 'Localisation', 'project', 'settings', 'and', 'created', 'tickets', 'accordingly'])]\n",
      "Summarize Text: \n",
      " Updated Native Knowledge Base for Local chatbot platform. Configured Briefing List and Shortcut buttons on Local chatbot platform Configured Greetings and Talk to Agent Card for Local chatbot platform. Local Chatbot launched. Performed UATs for Chatbot localisation project and raised ticket accordingly.Updated KTs and Card for Eid Shopping Fest Campaign, and raised ticket for failed cases for Chatbot localisation project. Updated KTs and Card for BNY-Eid Campaign\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "def read_article(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    filedata = file.readlines()\n",
    "    article = filedata[0].split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "\n",
    "# let's begin\n",
    "generate_summary( \"all_text.txt\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0606053452435824a427f0ea93d6ae20786397642025bcc66bfe96b66155d44b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
